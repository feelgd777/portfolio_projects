# Anton Smirnov's Portfolio

Hello, and thank you for stopping by! This repository contains examples of Python and SQL, as well as combinations of both, that are related to my data analytics projects. Below, you'll find a selection of highlighted projects, along with data visualizations I created with Tableau and matplotlib. Feel free to explore, and don't hesitate to [reach out](https://www.linkedin.com/in/anton-smirnov-89886332/).

## Highlighted Projects
* **[School cafeteria violations analysis](https://github.com/feelgd777/SQL_repo/blob/main/School%20cafeteria%20violations.md)** with SQL, Python, Tableau  
  In this project, I used a dataset found on Kaggle to analyze the nature and causes of School cafeteria violations in New York City boroughs. I focused on critical violations that pose a high risk to public health. My process entailed importing, cleaning, and organizing data and performing EDA in Python and SQL and visualizing with Tableau. 

* **[Walmart Sales Analaysis](https://github.com/feelgd777/SQL_repo/blob/main/Walmart%20Sales%20Analysis.md)** with SQL  
   In this project, I used a dataset found on Kaggle to analyze trends of Walmart sales. I created a database and used queries to filter data and answer business questions.

* **[US Airlines tweets sentiment analysis](https://github.com/feelgd777/SQL_repo/blob/main/Sentiment%20analysis%20of%20US%20Airlines%20tweets.md)** with Python and nltk  
  In this project, I used the nltk package in Python to process live tweets and classify the negative sentiments directed at US Airlines. Labeled data used for model training is on Kaggle. My process entailed nltk data cleaning, vectorization, model tuning, and building an API to analyze live tweets.

* **[Water pump functionality in Tanzania](https://github.com/feelgd777/SQL_repo/blob/main/Water%20Pump%20Functionality%20in%20Tanzania%20Readme.md)** with Python  
   In this project, I used the nltk package in Python to process live tweets and classify the negative sentiments directed at US Airlines. Labeled data used for model training can be found on Kaggle. My process entailed nltk data cleaning, vectorization, model tuning, and building an API to analyze live tweets.

